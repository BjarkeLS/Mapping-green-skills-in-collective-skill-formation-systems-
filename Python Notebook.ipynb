{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2660bf2d-c1a0-4fd4-a4bb-d1ad05f35554",
   "metadata": {},
   "source": [
    "# Python Notebook for \"Mapping Green Skills in Collective Skill Formation Systems: A Natural Language Processing Analysis of Danish Vocational Education and Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9088f1-f504-4516-98d7-d481c4cdc0b5",
   "metadata": {},
   "source": [
    "## Content:\n",
    "\n",
    "## 1. Loading documents and packages\n",
    "### 1.1Divide documents into sections\n",
    "\n",
    "## 2. Green dictionary search\n",
    "### 2.1 Purpose\n",
    "### 2.2. Competencies prior to admission\n",
    "### 2.3. Competencies in the main course\n",
    "### 2.4. Final exam\n",
    "\n",
    "## 3. Add educational fields and date\n",
    "\n",
    "## 4. Divide documents by competencies\n",
    "\n",
    "## 5. Separate final exam from competencies\n",
    "\n",
    "## 6. Classification and validation with LLM approach\n",
    "### 6.1 Validation\n",
    "\n",
    "## 7. Validation of LLM approach with manual review\n",
    "\n",
    "## 8. Add final exams to dataset\n",
    "\n",
    "## 9. Recode dataset - Rows are training ordinances instead of competencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334f801-3415-4501-891c-682f46a722dd",
   "metadata": {},
   "source": [
    "## 1. Loading documents and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f02e9-f7ff-4a83-b618-0e6b84552ae0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import pickle\n",
    "import spacy\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96053a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"(insert file location here)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b10e75-88ba-4cce-af9c-025970c86589",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        # Check if the file is a PDF\n",
    "        if file.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            try:\n",
    "                # Open the PDF file\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PdfReader(f)\n",
    "                    text = ''\n",
    "                    # Iterate through each page and extract text\n",
    "                    for page_num in range(len(reader.pages)):\n",
    "                        page = reader.pages[page_num]\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += page_text\n",
    "                    # Extract title from file name (without extension)\n",
    "                    title = os.path.splitext(file)[0]\n",
    "                    # Append the extracted text, title, and file path to the data list\n",
    "                    data.append({\n",
    "                        'Title': title,\n",
    "                        'Content': text,\n",
    "                        'File_Path': file_path  # New column for file path\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Title', 'Content', 'File_Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec919483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subfolder(file_path):\n",
    "    # Get the directory containing the file\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    # Get the name of the directory (subfolder)\n",
    "    subfolder = os.path.basename(dir_path)\n",
    "    return subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subfolder'] = df['File_Path'].apply(extract_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd552b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(content):\n",
    "    # Use a regular expression to find a date in the format 'DD/MM/YYYY'\n",
    "    match = re.search(r\"\\b(\\d{2}/\\d{2}/\\d{4})\\b\", content)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the matched date\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e11bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Content'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(content):\n",
    "    # Regular expression to match the footer structure\n",
    "    footer_pattern = r\"BEK nr \\d{1,4} af \\d{2}/\\d{2}/\\d{4} \\d\"\n",
    "    # Use re.sub() to remove the matched footer from the content\n",
    "    return re.sub(footer_pattern, '', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f39618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].apply(remove_footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e97fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015 = df[~df['File_Path'].str.contains(\"Kategoriserede uddannelser\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a404a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607714a9-492b-471f-9721-9a78081eee49",
   "metadata": {},
   "source": [
    "### 1.1 Divide documents into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e06412-3b62-405b-8ac7-b03c2b5eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_formål(content, Title):\n",
    "    # Check if the title is \"B20170033205\" to apply the special rule\n",
    "    if Title == \"B20170033205\":\n",
    "        # Use regular expression for the specific rule for this title\n",
    "        match = re.search(r\"Formål og opdeling(.*?)Uddannelsen udbydes med talentspor.\", content, re.DOTALL)\n",
    "    else:\n",
    "        # Original rule for other rows\n",
    "        match = re.search(r\"Formål og opdeling(.*?)Varighed\", content, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return the matched content (group 2), stripping any surrounding whitespace\n",
    "    else:\n",
    "        return None  # Return None if the pattern is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['formål_og_opdeling'] = df_e2015.apply(lambda row: extract_formål(row['Content'], row['Title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "formål = df_e2015.loc[729, 'formål_og_opdeling'][:1000]\n",
    "print(formål)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2315656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kompetencer_forud(content, Title):\n",
    "    # Check if the title is \"B20170033205\" to apply the special rule\n",
    "    if Title == \"B20170033205\":\n",
    "        # Use regular expression for the specific rule for this title\n",
    "        match = re.search(r\"(1, nr. 2, i lov om erhvervsuddannelser.)(.*?)(Kompetencer i hovedforløbet)\", content, re.DOTALL)\n",
    "    else:\n",
    "        # Original rule for other rows\n",
    "        match = re.search(r\"(Kompetencer forud for optagelse på det studiekompetencegivende forløb og forløbets indhold|Kompetencer forud for optagelse til skoleundervisning i hovedforløbet|Kompetencer forud for optagelse til skoleundervisningen i hovedforløbet)(.*?)(Kompetencer i hovedforløbet|Kompetencer m\\.v\\. i hovedforløbet|Kompetencer mv\\. i hovedforløbet)\", content, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(2).strip()  # Return the matched content (group 2), stripping any surrounding whitespace\n",
    "    else:\n",
    "        return None  # Return None if the pattern is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d978d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['kompetencer_forud_optagelse'] = df_e2015.apply(lambda row: extract_kompetencer_forud(row['Content'], row['Title']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kompetencer_hovedforløb(content):\n",
    "    # Use regular expression to find text between \"Kompetencer i hovedforløbet\" or \"Kompetencer m.v. i hovedforløbet\" and \"Godskrivning og merit\"\n",
    "    match = re.search(r\"(Kompetencer i hovedforløbet|Kompetencer m\\.v\\. i hovedforløbet|Kompetencer mv\\. i hovedforløbet)(.*?)(Godskrivning)\", content, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(2).strip()  # Return the content between the matches, stripping any surrounding whitespace\n",
    "    else:\n",
    "        return None  # Return None if the pattern is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['kompetencer_hovedforløb'] = df_e2015['Content'].apply(extract_kompetencer_hovedforløb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37747a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_afsluttende_prøve(content):\n",
    "    # Regular expression to find text between the two sets of keywords\n",
    "    match = re.search(r\"(Afsluttende prøve og gymnasial eksamen|Afsluttende prøve og svendeprøve|Afsluttende prøve (svendeprøve)|Afsluttende prøve|Afsluttende prøve, svendeprøve og gymnasial eksamen|Svendeprøve og gymnasial eksamen (erhvervsfaglig studentereksamen)|Svendeprøve og gymnasial eksamen|Fagprøve og gymnasial eksamen|Fagprøve|Svendeprøve)(.*?)(Beviser|Ikrafttrædelse og overgangsbestemmelser)\", content, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(2).strip()  # Return the content between the matches, stripping any surrounding whitespace\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_afsluttende_prøve(content):\n",
    "    # Use regular expression to find the text between the starting and ending phrases\n",
    "    match = re.search(r\"(Afsluttende prøve og gymnasial eksamen|Afsluttende prøve og svendeprøve|Afsluttende prøve \\(svendeprøve\\)|Afsluttende prøve|Afsluttende prøve, svendeprøve og gymnasial eksamen|Svendeprøve og gymnasial eksamen \\(erhvervsfaglig studentereksamen\\)|Svendeprøve og gymnasial eksamen|Fagprøve og gymnasial eksamen|Fagprøve|Svendeprøve)(.*?)(Beviser|Ikrafttrædelse og overgangsbestemmelser)\", content, re.DOTALL)\n",
    "    \n",
    "    # Check if a match was found\n",
    "    if match:\n",
    "        return match.group(2).strip()  # Return the matched content, stripping any surrounding whitespace\n",
    "    else:\n",
    "        return None  # Return None if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf4601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['afsluttende_prøve'] = df_e2015['Content'].apply(extract_afsluttende_prøve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea3542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bd8ee-6808-43d6-ad9b-47749a47a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015.to_pickle('df_e2015_ny.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc9a01",
   "metadata": {},
   "source": [
    "## 2. Green dictionary search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6459cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('da_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57890c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['luftkvalitet', 'akvakultur', 'bionedbryd', 'biodivers', 'biobrændstof', 'biomasse', \n",
    "             'co2', 'cirkulær', 'ren energi', 'klima', 'bevarelse', 'katastroferisikostyring', \n",
    "             'energieffekt', 'energihandel', 'miljø', 'geotermisk', 'gletsjerforsk', 'grøn', \n",
    "             'hydrologi', 'arealanvend', 'landskabsarkitekt', 'LEED', 'leadership in energy and environmental design',\n",
    "             'naturlige pesticider', 'naturforsk', \n",
    "             'foruren', 'genbrug', 'vedvarende', 'solenergi', 'håndtering af affald', 'behandling af affald', 'affaldshåndt',\n",
    "             'affaldsbehandl', 'affaldssort', 'bæredygtig', 'affaldsredu', 'spildevand', \n",
    "             'spildevandsreduktion', 'behandling af spildevand', 'reduktion af spildevand', 'vandressource', \n",
    "             'vejrsikring', 'dyreliv', 'vindenergi', 'udled', 'drivgas', 'ressourceeffekt', \n",
    "             'reduktion af material', 'vandforbrug', 'regenerativ', 'genanvend', \n",
    "             'giftige kemikalie', 'giftige stof', 'EPD', 'livscyklus', 'LCA', 'vedligehold', 'energioptim', \n",
    "             'ressourceanvende', 'energikild', 'energiforbrug', 'madspild', 'ressourceforbrug', \n",
    "             'kemikaliesikkerhed', 'vildtplej', 'naturplej', 'naturbeskytte', 'beskyt natur', 'naturbevar', \n",
    "             'bevare natur', 'naturudvikling', 'udvikle natur', 'økologi', 'økosystem', 'skovpleje', 'pleje af natur',\n",
    "             'pleje af skov', 'pleje af vild', 'biotoppleje', 'træpleje', 'ressourcebespar', 'ressourcebevid',\n",
    "             'ressourceanvende', 'materialespild', 'ressourcespild', 'spild af materiale', 'spild af ressource', 'repar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b86274-34b4-47e6-87ac-ad647c6940c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = {\n",
    "    \"arbejdsmiljø\", \"bejdsmiljø\", \"netværksmiljø\", \"kontormiljø\", \"bomiljøer\", \n",
    "    \"staldmiljø\", \"værksmiljø\", \"gågademiljø\", \"læringsmiljø\", \"grøntsager\", \n",
    "    \"grønsag\", \"pyntegrønt\", \"klimasystemer\", \"klimaanlæg\", \"indeklima\", \"miljøministeriets\", \"arbejds miljø\"\n",
    "    \"klima og ventilationsanlæg\", \"klimaog ventilationsanlæg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb23e17",
   "metadata": {},
   "source": [
    "### 2.1 Purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove hyphen followed by a line break and join the split word\n",
    "    # This regex looks for hyphens followed by a line break or space and joins the words\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)  # Handles hyphenated line breaks\n",
    "    \n",
    "    # Remove other punctuation and non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Use spaCy to process and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize each token while preserving case of acronyms\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe73f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['cleaned_formål_og_opdeling'] = df_e2015['formål_og_opdeling'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    matches = []\n",
    "    \n",
    "    # First, check for multi-word terms directly in the text (without splitting)\n",
    "    for word in word_list:\n",
    "        # Handle multi-word terms (case-insensitive match)\n",
    "        if \" \" in word:  # This checks if the word in the list is a phrase\n",
    "            if word.lower() in text.lower():  # Check for multi-word phrases in the text\n",
    "                matches.append(word)\n",
    "    \n",
    "    # Then, split the text into words for more granular matching for single words\n",
    "    words_in_text = text.split()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if \" \" not in word:  # This only handles single-word terms\n",
    "            # Check each word in the text\n",
    "            for text_word in words_in_text:\n",
    "                # Check if the stopword is a part of the current word in the text\n",
    "                if not any(stopword in text_word.lower() for stopword in stopword_list):\n",
    "                    # For acronyms like \"EPD\", use case-sensitive search; for others, search case-insensitively\n",
    "                    if word.isupper() and word in text_word:  # Case-sensitive search for acronyms\n",
    "                        matches.append(word)\n",
    "                    elif word.lower() in text_word.lower():  # Case-insensitive search for non-acronyms\n",
    "                        matches.append(word)\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['grøn_formål_og_opdeling'] = df_e2015['cleaned_formål_og_opdeling'].apply(find_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30dd60c",
   "metadata": {},
   "source": [
    "### 2.2. Competencies prior to admission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove hyphen followed by a line break and join the split word\n",
    "    # This regex looks for hyphens followed by a line break or space and joins the words\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)  # Handles hyphenated line breaks\n",
    "    \n",
    "    # Remove other punctuation and non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Use spaCy to process and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize each token while preserving case of acronyms\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a1dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['cleaned_kompetencer_forud_optagelse'] = df_e2015['kompetencer_forud_optagelse'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    matches = []\n",
    "    \n",
    "    # First, check for multi-word terms directly in the text (without splitting)\n",
    "    for word in word_list:\n",
    "        # Handle multi-word terms (case-insensitive match)\n",
    "        if \" \" in word:  # This checks if the word in the list is a phrase\n",
    "            if word.lower() in text.lower():  # Check for multi-word phrases in the text\n",
    "                matches.append(word)\n",
    "    \n",
    "    # Then, split the text into words for more granular matching for single words\n",
    "    words_in_text = text.split()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if \" \" not in word:  # This only handles single-word terms\n",
    "            # Check each word in the text\n",
    "            for text_word in words_in_text:\n",
    "                # Check if the stopword is a part of the current word in the text\n",
    "                if not any(stopword in text_word.lower() for stopword in stopword_list):\n",
    "                    # For acronyms like \"EPD\", use case-sensitive search; for others, search case-insensitively\n",
    "                    if word.isupper() and word in text_word:  # Case-sensitive search for acronyms\n",
    "                        matches.append(word)\n",
    "                    elif word.lower() in text_word.lower():  # Case-insensitive search for non-acronyms\n",
    "                        matches.append(word)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c9d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['grøn_kompetencer_forud_optagelse'] = df_e2015['cleaned_kompetencer_forud_optagelse'].apply(find_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81668164",
   "metadata": {},
   "source": [
    "### 2.3. Competencies in the main course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd69450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove hyphen followed by a line break and join the split word\n",
    "    # This regex looks for hyphens followed by a line break or space and joins the words\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)  # Handles hyphenated line breaks\n",
    "    \n",
    "    # Remove punctuation and non-alphanumeric characters, except parentheses\n",
    "    text = re.sub(r'[^\\w\\s\\(\\)]', '', text)\n",
    "\n",
    "    # Use spaCy to process and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize each token while preserving case of acronyms\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf79f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['cleaned_kompetencer_hovedforløb'] = df_e2015['kompetencer_hovedforløb'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    matches = []\n",
    "    \n",
    "    # First, check for multi-word terms directly in the text (without splitting)\n",
    "    for word in word_list:\n",
    "        # Handle multi-word terms (case-insensitive match)\n",
    "        if \" \" in word:  # This checks if the word in the list is a phrase\n",
    "            if word.lower() in text.lower():  # Check for multi-word phrases in the text\n",
    "                matches.append(word)\n",
    "    \n",
    "    # Then, split the text into words for more granular matching for single words\n",
    "    words_in_text = text.split()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if \" \" not in word:  # This only handles single-word terms\n",
    "            # Check each word in the text\n",
    "            for text_word in words_in_text:\n",
    "                # Check if the stopword is a part of the current word in the text\n",
    "                if not any(stopword in text_word.lower() for stopword in stopword_list):\n",
    "                    # For acronyms like \"EPD\", use case-sensitive search; for others, search case-insensitively\n",
    "                    if word.isupper() and word in text_word:  # Case-sensitive search for acronyms\n",
    "                        matches.append(word)\n",
    "                    elif word.lower() in text_word.lower():  # Case-insensitive search for non-acronyms\n",
    "                        matches.append(word)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6554ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['grøn_kompetencer_hovedforløb'] = df_e2015['cleaned_kompetencer_hovedforløb'].apply(find_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf649cf",
   "metadata": {},
   "source": [
    "### 2.4. Final exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12540c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove hyphen followed by a line break and join the split word\n",
    "    # This regex looks for hyphens followed by a line break or space and joins the words\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)  # Handles hyphenated line breaks\n",
    "    \n",
    "    # Remove other punctuation and non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Use spaCy to process and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize each token while preserving case of acronyms\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f435b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['cleaned_afsluttende_prøve'] = df_e2015['afsluttende_prøve'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    matches = []\n",
    "    \n",
    "    # First, check for multi-word terms directly in the text (without splitting)\n",
    "    for word in word_list:\n",
    "        # Handle multi-word terms (case-insensitive match)\n",
    "        if \" \" in word:  # This checks if the word in the list is a phrase\n",
    "            if word.lower() in text.lower():  # Check for multi-word phrases in the text\n",
    "                matches.append(word)\n",
    "    \n",
    "    # Then, split the text into words for more granular matching for single words\n",
    "    words_in_text = text.split()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if \" \" not in word:  # This only handles single-word terms\n",
    "            # Check each word in the text\n",
    "            for text_word in words_in_text:\n",
    "                # Check if the stopword is a part of the current word in the text\n",
    "                if not any(stopword in text_word.lower() for stopword in stopword_list):\n",
    "                    # For acronyms like \"EPD\", use case-sensitive search; for others, search case-insensitively\n",
    "                    if word.isupper() and word in text_word:  # Case-sensitive search for acronyms\n",
    "                        matches.append(word)\n",
    "                    elif word.lower() in text_word.lower():  # Case-insensitive search for non-acronyms\n",
    "                        matches.append(word)\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd17f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e2015['grøn_afsluttende_prøve'] = df_e2015['cleaned_afsluttende_prøve'].apply(find_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015.to_pickle('df_e2015_ny.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d15001-16c5-45aa-93a5-fa0b7d0bf596",
   "metadata": {},
   "source": [
    "## 3. Add educational fields and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18880821-32d4-4e88-97eb-94efb6f1a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015.rename(columns={'Subfolder': 'uddannele'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e99dd8-9f24-4f2d-9daa-e1b970c931cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_excel('hovedområder.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fc521-6427-4d48-97da-c80da8d58aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df.columns = ['uddannele', 'hovedområde']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f427489-ecaf-4258-989e-19f4a3541e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['uddannele'] = df_e2015['uddannele'].str.lower()\n",
    "mapping_df['uddannele'] = mapping_df['uddannele'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1749ca3-840d-4676-adc4-df6e6abf1e4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_e2015 = df_e2015.merge(mapping_df, on='uddannele', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca93c8-8df7-4726-a5b0-4438cac59322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['Date'] = pd.to_datetime(df_e2015['Date'], format='%d/%m/%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231d379-c1b8-451d-bbe1-0292458c7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015['year'] = df_e2015['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591dc33b-182d-4fa9-98e4-c8c0d1c358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015 = df_e2015.sort_values('Date').drop_duplicates(subset=['uddannele', 'year'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39803d2c-62f7-4bcd-aa07-70de32d1ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_timeline(df):\n",
    "    # Define the timeline range from the first year to 2024\n",
    "    min_year = df['year'].min()\n",
    "    max_year = 2024  # Ensure the timeline goes up to 2024\n",
    "    \n",
    "    # Create a DataFrame with a row for each year within the range for each education\n",
    "    all_years = pd.DataFrame({'year': range(min_year, max_year + 1)})\n",
    "    \n",
    "    # Merge the new years DataFrame with the existing data, keeping all years\n",
    "    df = pd.merge(all_years, df, on='year', how='left')\n",
    "    \n",
    "    # Fill missing rows with data from the previous year (forward fill)\n",
    "    df = df.ffill()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a121ed-5825-4212-b855-0d27e2b54cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015 = (\n",
    "    df_e2015.groupby('uddannele', group_keys=False)\n",
    "    .apply(create_annual_timeline)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d023dda-2fc1-4176-9311-55578a31b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015 = df_e2015.sort_values(['uddannele', 'year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82deaa-4263-421e-ad0b-a0e3f6a14cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = \"DISCED gruppering af EUD.xlsx\"\n",
    "df_DISCED = pd.read_excel(excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab36bdc-8e14-49c4-912e-bec4263ca1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015 = df_e2015.merge(df_DISCED[['uddannele', 'Område_Omkodet']], on='uddannele', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644a06a-b278-45a5-ab20-7e15ad91b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2015.to_pickle('df_e2015_f_WE.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9483-3850-4e5b-b6cd-dd266ee46356",
   "metadata": {},
   "source": [
    "## 4. Divide documents by competencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdba585-dd04-46d9-8651-0ad078de4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672934a8-52d9-4e6e-92de-d0be0e72dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skill_descriptions_formål(text):\n",
    "    # Updated regex to capture digits followed by a parenthesis with or without a space, e.g., \"8)\" or \"8 )\"\n",
    "    pattern = r'(\\d{1,2}\\s?\\))\\s*(.*?)(?=\\d{1,2}\\s?\\)|stk \\d|$)'\n",
    "    \n",
    "    # Extract skill descriptions based on the updated pattern\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if matches:\n",
    "        # Extract just the descriptions (ignore the numbers and closing parentheses)\n",
    "        skill_descriptions = [match[1].strip() for match in matches]\n",
    "    else:\n",
    "        # If no matches, take all text before \"stk 2\" as one skill\n",
    "        if \"stk 2\" in text:\n",
    "            skill_descriptions = [text.split(\"stk 2\")[0].strip()]\n",
    "        else:\n",
    "            skill_descriptions = [text.strip()]  # If no \"stk 2\", take all text as one skill\n",
    "    \n",
    "    return skill_descriptions\n",
    "\n",
    "# Generic extract_skill_descriptions function for other sections\n",
    "def extract_skill_descriptions(text):\n",
    "    # Regex to capture digits followed by a parenthesis with or without a space\n",
    "    pattern = r'(\\d{1,2}\\s?\\))\\s*(.*?)(?=\\d{1,2}\\s?\\)|$)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    skill_descriptions = [match[1].strip() for match in matches]\n",
    "    return skill_descriptions\n",
    "\n",
    "# Function for cleaned_formål_og_opdeling\n",
    "def divide_formål_og_opdeling(text):\n",
    "    if \"stk 2\" in text:\n",
    "        text = text.split(\"stk 2\")[0]\n",
    "    return extract_skill_descriptions_formål(text)\n",
    "\n",
    "# Function for cleaned_kompetencer_forud_optagelse with exclusions\n",
    "exclude_keywords = [\"aniveau\", \"bniveau\", \"cniveau\", \"dniveau\", \"eniveau\", \"fniveau\", \"gniveau\"]\n",
    "\n",
    "def divide_kompetencer_forud_optagelse(text):\n",
    "    skill_descriptions = extract_skill_descriptions(text)\n",
    "    filtered_skills = [\n",
    "        skill for skill in skill_descriptions\n",
    "        if not any(keyword in skill.lower() for keyword in exclude_keywords)\n",
    "    ]\n",
    "    return filtered_skills\n",
    "\n",
    "# Function for cleaned_kompetencer_hovedforløb\n",
    "def divide_kompetencer_hovedforløb(text):\n",
    "    if \"stk 2\" in text:\n",
    "        text = text.split(\"stk 2\")[0]\n",
    "    return extract_skill_descriptions(text)\n",
    "\n",
    "# Initialize the list for new rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over each row in the original DataFrame\n",
    "for _, row in df_e2015.iterrows():\n",
    "    # Extract and divide sections\n",
    "    sections = {\n",
    "        \"cleaned_formål_og_opdeling\": divide_formål_og_opdeling(row[\"cleaned_formål_og_opdeling\"]),\n",
    "        \"cleaned_kompetencer_forud_optagelse\": divide_kompetencer_forud_optagelse(row[\"cleaned_kompetencer_forud_optagelse\"]),\n",
    "        \"cleaned_kompetencer_hovedforløb\": divide_kompetencer_hovedforløb(row[\"cleaned_kompetencer_hovedforløb\"]),\n",
    "        \"cleaned_afsluttende_prøve\": [row[\"cleaned_afsluttende_prøve\"]]  # One row per original\n",
    "    }\n",
    "\n",
    "    # Loop through each section and its extracted skills\n",
    "    for section_name, skills in sections.items():\n",
    "        for skill in skills:\n",
    "            new_rows.append({\n",
    "                \"year\": row[\"year\"],\n",
    "                \"uddannele\": row[\"uddannele\"],\n",
    "                \"Date\": row[\"Date\"],\n",
    "                \"hovedområde\": row[\"hovedområde\"],\n",
    "                \"Område_Omkodet\": row[\"Område_Omkodet\"],\n",
    "                \"section\": section_name,\n",
    "                \"skill_description\": skill\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2695db5-5282-48e2-8eea-27d98b9415b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab3744-54cd-4f9f-95f3-32fa297a1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('da_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589b052-f3a1-488c-b643-c05823ea476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['luftkvalitet', 'akvakultur', 'bionedbryd', 'biodivers', 'biobrændstof', 'biomasse', \n",
    "             'co2', 'cirkulær', 'ren energi', 'klima', 'bevarelse', 'katastroferisikostyring', \n",
    "             'energieffekt', 'energihandel', 'miljø', 'geotermisk', 'gletsjerforsk', 'grøn', \n",
    "             'hydrologi', 'arealanvend', 'landskabsarkitekt', 'LEED', 'leadership in energy and environmental design',\n",
    "             'naturlige pesticider', 'naturforsk', \n",
    "             'foruren', 'genbrug', 'vedvarende', 'solenergi', 'håndtering af affald', 'behandling af affald', 'affaldshåndt',\n",
    "             'affaldsbehandl', 'affaldssort', 'bæredygtig', 'affaldsredu', 'spildevand', \n",
    "             'spildevandsreduktion', 'behandling af spildevand', 'reduktion af spildevand', 'vandressource', \n",
    "             'vejrsikring', 'dyreliv', 'vindenergi', 'udled', 'drivgas', 'ressourceeffekt', \n",
    "             'reduktion af material', 'vandforbrug', 'regenerativ', 'genanvend', \n",
    "             'giftige kemikalie', 'giftige stof', 'EPD', 'livscyklus', 'LCA', 'vedligehold', 'energioptim', \n",
    "             'ressourceanvende', 'energikild', 'energiforbrug', 'madspild', 'ressourceforbrug', \n",
    "             'kemikaliesikkerhed', 'vildtplej', 'naturplej', 'naturbeskytte', 'beskyt natur', 'naturbevar', \n",
    "             'bevare natur', 'naturudvikling', 'udvikle natur', 'økologi', 'økosystem', 'skovpleje', 'pleje af natur',\n",
    "             'pleje af skov', 'pleje af vild', 'biotoppleje', 'træpleje', 'ressourcebespar', 'ressourcebevid',\n",
    "             'ressourceanvende', 'materialespild', 'ressourcespild', 'spild af materiale', 'spild af ressource', 'repar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0938cee-32bd-4c2a-897b-885415669512",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = {\n",
    "    \"arbejdsmiljø\", \"bejdsmiljø\", \"netværksmiljø\", \"kontormiljø\", \"bomiljøer\", \n",
    "    \"staldmiljø\", \"værksmiljø\", \"gågademiljø\", \"læringsmiljø\", \"grøntsager\", \n",
    "    \"grønsag\", \"pyntegrønt\", \"klimasystemer\", \"klimaanlæg\", \"indeklima\", \"miljøministeriets\", \"arbejds miljø\"\n",
    "    \"klima og ventilationsanlæg\", \"klimaog ventilationsanlæg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce8fa9-e26d-47e7-87eb-ef0f83e686c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    matches = []\n",
    "    \n",
    "    # Lowercase text for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    words_in_text = text.split()\n",
    "\n",
    "    # Check for multi-word terms in the text\n",
    "    for word in word_list:\n",
    "        if \" \" in word:  # Multi-word terms\n",
    "            if word.lower() in text_lower:\n",
    "                matches.append(word)\n",
    "    \n",
    "    # Check for single-word terms in the text\n",
    "    for word in word_list:\n",
    "        if \" \" not in word:  # Single-word terms only\n",
    "            for text_word in words_in_text:\n",
    "                if not any(stopword in text_word.lower() for stopword in stopword_list):\n",
    "                    if word.isupper() and word in text_word:  # Acronyms: Case-sensitive\n",
    "                        matches.append(word)\n",
    "                    elif word.lower() in text_word.lower():  # Regular words: Case-insensitive\n",
    "                        matches.append(word)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42efa7-66d6-4010-be8c-5ea4f45af692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['grøn_words'] = df_new['skill_description'].apply(find_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c72a5c-8bc9-4b4c-8c8a-53f7be7c740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['is_green'] = df_new['grøn_words'].apply(lambda x: bool(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9375592-9d56-4840-b3b4-eaf2d0e52706",
   "metadata": {},
   "source": [
    "## 5. Separate final exam from competencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c872dc-4c76-4d15-a64c-9b26514bb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new[df_new['section'] != \"cleaned_afsluttende_prøve\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656ac96-4927-4071-8f56-28d62093b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "prøve = df_new[df_new['section'] == 'cleaned_afsluttende_prøve']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacddf5c-721d-466f-9f97-dbbb290a8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prøve.rename(columns={'grøn_words': 'grøn_words_final'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365618b-d952-4b62-882c-99c551d06e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prøve['green_or_not'] = prøve['grøn_words_final'].apply(lambda x: False if not x else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b944eda-9658-4b0d-803a-037ac6f53a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prøve.to_pickle('prøve.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8ae50-813f-4269-b541-f98c66a7320e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_new.to_pickle('df_e2015_f_WE_3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3ab3c-68f8-4521-8287-05a23c7f664c",
   "metadata": {},
   "source": [
    "## 6. Classification and validation with LLM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6434ca-22c7-4727-a459-1d76116e5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c068d2-4d3b-4472-bd57-afc4ac0854e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(row):\n",
    "    prompt = format_prompt(row)\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da2414-b9ea-43f2-8cfe-4e6df81be47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['token_count'] = df_new.apply(calculate_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571f957-c234-45d3-9384-0dcd976ce299",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_limit = 3096\n",
    "\n",
    "# Filter rows exceeding the limit\n",
    "rows_exceeding_limit = df_new[df_new['token_count'] > input_token_limit]\n",
    "\n",
    "# Display rows exceeding the limit\n",
    "print(f\"Number of rows exceeding token limit: {len(rows_exceeding_limit)}\")\n",
    "rows_exceeding_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0cbfa-71e1-4ea7-8b7a-891e5ce01ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda0c8e-c8d3-40b8-908a-8a45e0c9a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8e2f5-b967-450a-a1e8-4a0c3eb218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='(OpenAI KEY insert here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723aa49-451f-44fd-ab99-4d937cba32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_count = 0\n",
    "max_errors_to_print = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cd2b2-bec6-4a55-9ee9-cb1f3a93f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_skill(row):\n",
    "    global error_count  # Use the global error counter\n",
    "\n",
    "    # Perform dictionary-based classification\n",
    "    is_green = len(row['grøn_words']) > 0  # Based on dictionary approach\n",
    "\n",
    "    # Create the prompt for classification\n",
    "    classification_prompt = f\"\"\"\n",
    "    Skill description: \"{row['skill_description']}\"\n",
    "    Year: \"{row['year']}\"\n",
    "    Education: \"{row['uddannele']}\"\n",
    "    Category: \"{row['Område_Omkodet']}\"\n",
    "    Green words detected: \"{', '.join(row['grøn_words']) if row['grøn_words'] else 'None'}\"\n",
    "    Question: Er denne færdighedsbeskrivelse relateret til grønne færdigheder? \n",
    "    Grønne færdigheder defineres bredt som personlige kvaliteter, færdigheder, viden, evner og aktiviteter, \n",
    "    der bidrager til bæredygtig udvikling ved 1) at reducere energiforbrug, \n",
    "    2) beskytte økosystemer eller biodiversitet eller \n",
    "    3) minimere affald eller emissioner. \n",
    "    Start dit svar med 'Ja' eller 'Nej'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the classification from GPT-4o\n",
    "        classification_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Specify the GPT-4o model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du er en ekspert i miljømæssig bæredygtighed og uddannelse.\"},\n",
    "                {\"role\": \"user\", \"content\": classification_prompt}\n",
    "            ],\n",
    "            temperature=0.5,  # Adjust for deterministic outputs\n",
    "        )\n",
    "        # Extract the model's classification\n",
    "        classification_text = classification_response.choices[0].message.content.strip()\n",
    "        is_green_model = classification_text.lower().startswith(\"ja\")\n",
    "\n",
    "        # Check if the classifications differ\n",
    "        explanation = None\n",
    "        if is_green_model != is_green:\n",
    "            # Create the explanation prompt\n",
    "            explanation_prompt = f\"\"\"\n",
    "            Skill description: \"{row['skill_description']}\"\n",
    "            Year: \"{row['year']}\"\n",
    "            Education: \"{row['uddannele']}\"\n",
    "            Category: \"{row['Område_Omkodet']}\"\n",
    "            Green words detected: \"{', '.join(row['grøn_words']) if row['grøn_words'] else 'None'}\"\n",
    "            Question: Forklar kort hvorfor denne færdighedsbeskrivelse er {('ikke ' if not is_green_model else '')}relateret til grønne færdigheder.\n",
    "            \"\"\"\n",
    "            explanation_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Du er en ekspert i miljømæssig bæredygtighed og uddannelse.\"},\n",
    "                    {\"role\": \"user\", \"content\": explanation_prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            # Extract the explanation\n",
    "            explanation = explanation_response.choices[0].message.content.strip()\n",
    "\n",
    "        return is_green_model, explanation\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error for the first 10 errors only\n",
    "        if error_count < max_errors_to_print:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "        error_count += 1\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55ddcb-52f1-4719-b8d3-8bc624218ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['is_green_classification', 'chatgpt_response']] = df_new.apply(\n",
    "    lambda row: pd.Series(classify_skill(row)), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad07248-7006-4270-b37a-3937bfd9e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_pickle('df_e2015_WE_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cd4a8-1cdf-47de-9414-befba4cb37db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839cf8d-5ca1-4aae-8ad1-46ee41f8e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdec8f9-dd46-422c-8175-159b03d92b50",
   "metadata": {},
   "source": [
    "### 6.1 Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86d4a5-942c-4aac-9abc-1acb6ba39dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(df_new['is_green'], df_new['is_green_classification'])\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(df_new['is_green'], df_new['is_green_classification'])\n",
    "\n",
    "# Calculate the sensitivity (recall)\n",
    "sensitivity = recall_score(df_new['is_green'], df_new['is_green_classification'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708a5c4-d567-4f24-bfd9-672ad0e67635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity}\")\n",
    "print(f\"Precision: {precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af4555-381c-415c-9ba1-98da2b337496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fcfea-7585-4c52-a0b3-b59e56c3ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(df_new['is_green'], df_new['is_green_classification']).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8b844-894b-4b9e-b506-7c21a189049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print specificity\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2abd4-bc14-4db5-af9b-58ec0f5ae0ef",
   "metadata": {},
   "source": [
    "## 7. Validation of LLM approach with manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702002cb-4df2-4be1-81ac-9b529476cdca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_df = pd.read_excel('df_WE.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec0488-b22f-4784-bb79-9a8f0dcdf3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.reset_index()\n",
    "updated_df = updated_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddac9d3-dca5-45b8-9875-e31961eecc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = updated_df[updated_df['final_assessment'] == 'x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a62fae-e0fb-4644-a8d9-25b607e0f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['chatgpt_response'] = filtered_df.apply(\n",
    "    lambda row: \n",
    "    df_new.loc[\n",
    "        (df_new['skill_description'] == row['skill_description']) &\n",
    "        (df_new['section'] == row['section']) &\n",
    "        (df_new['year'] == row['year']) &\n",
    "        (df_new['uddannele'] == row['uddannele']),\n",
    "        'chatgpt_response'\n",
    "    ].values[0] if pd.isna(row['chatgpt_response']) or row['chatgpt_response'] == '' else row['chatgpt_response'],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c8bad-5812-4bae-a454-37de34436994",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.to_excel(\"filtered_with_updated_chatgpt_response.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8e03-b007-4093-a8d5-fa7735ece2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['final_assessment'] = df_new.apply(\n",
    "    lambda row: \n",
    "    filtered_df.loc[\n",
    "        (filtered_df['chatgpt_response'] == row['chatgpt_response']),\n",
    "        'final_assessment'\n",
    "    ].values[0] if row['chatgpt_response'] in filtered_df['chatgpt_response'].values else None,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b7ed2-c451-4831-9de9-c874ae71cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['green_or_not'] = df_new.apply(\n",
    "    lambda row: row['is_green_classification'] if row['final_assessment'] == 'x' else row['is_green'],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed472c12-62ad-4dab-ab0c-0b2007918df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['grøn_words_final'] = df_new.apply(\n",
    "    lambda row: [] if row['final_assessment'] == 'x' else row['grøn_words'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdaed32-dfb5-4bce-b21e-dd7c0577f306",
   "metadata": {},
   "source": [
    "## 8. Add final exams to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c6a71-d1df-43ff-b2f8-484c83a18686",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prøve.pkl', 'rb') as file:\n",
    "    prøve = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fefce-a1d2-43ba-97c1-4edbdaf9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_final = pd.concat([df_new, prøve], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c8ae1-dfba-41e5-9ed7-ad096c696e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_final.to_pickle('df_e2015_WE_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb81090-ea2d-4a1d-ad3a-29902994fcc9",
   "metadata": {},
   "source": [
    "## 9. Recode dataset - Rows are training ordinances instead of competencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549423a-5419-46d0-92a3-53dc8dfabaec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "educations = df_new_final.groupby(['uddannele', 'year'], as_index=False).agg({\n",
    "    'Date': 'first',  # Keep the first date (or adjust logic as needed)\n",
    "    'hovedområde': 'first',  # Keep the first hovedområde (or adjust logic)\n",
    "    'Område_Omkodet': 'first',  # Keep the first Område_Omkodet (or adjust logic)\n",
    "    'green_or_not': 'mean'  # Calculate the mean of 'green_or_not' for green_outcome\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cca5c-b631-43aa-aaff-decc5651da97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "educations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1a261-9bb0-468a-af07-de30940d1947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
